{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Chat Completions API\n",
    "https://platform.openai.com/docs/overview  \n",
    "https://platform.openai.com/docs/api-reference/chat  \n",
    "\n",
    "배포된 openai의 api key를 .env의 OPENAI_API_KEY에 등록하여 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chat completion 요청(Request) param 정리\n",
    "### POST https://api.openai.com/v1/chat/completions\n",
    "\n",
    "| Param                    | Type                     | Req | Default        | 설명                                                   |\n",
    "| ------------------------ | ------------------------ | --: | -------------- | ---------------------------------------------------- |\n",
    "| `model`                  | string                   |   ✅ | -              | 사용할 모델 ID                                            |\n",
    "| `messages`               | array                    |   ✅ | -              | 대화 메시지 배열(이전 대화 포함). 각 항목은 `role`, `content` 등을 포함   |\n",
    "| `temperature`            | number                   |     | -              | 샘플링 랜덤성(높을수록 다양/창의). 보통 `top_p`와 둘 중 하나만 조정          |\n",
    "| `top_p`                  | number                   |     | -              | Nucleus sampling. 확률 질량 상위 `top_p` 범위에서 샘플링          |\n",
    "| `max_completion_tokens`  | integer | null           |     | -              | 생성 토큰 상한(출력 토큰 + reasoning 토큰 포함)                    |\n",
    "| `max_tokens`             | integer | null           |     | -              | **Deprecated**. `max_completion_tokens` 사용 권장        |\n",
    "| `n`                      | integer                  |     | 1              | 생성할 응답 후보(choices) 개수                                |\n",
    "| `stop`                   | string | string[] | null |     | null           | 최대 4개의 중단 시퀀스. 해당 문자열을 만나면 생성 중단(일부 모델 미지원 가능)       |\n",
    "| `stream`                 | boolean                  |     | false          | true면 SSE로 스트리밍 응답                                   |\n",
    "| `stream_options`         | object                   |     | -              | 스트리밍 옵션(예: `include_usage` 등)                        |\n",
    "| `response_format`        | object                   |     | -              | 출력 형식 강제(예: `json_schema`, `json_object`)            |\n",
    "| `tools`                  | array                    |     | -              | 모델이 호출 가능한 도구 목록(function tool 등)                    |\n",
    "| `tool_choice`            | string | object          |     | `auto`(모델에 따라) | 도구 호출 방식 제어(`none`, `auto`, `required`, 특정 도구 강제 등)  |\n",
    "| `functions`              | array                    |     | -              | **Deprecated**. `tools`로 대체                          |\n",
    "| `function_call`          | string | object          |     | -              | **Deprecated**. `tool_choice`로 대체                    |\n",
    "| `frequency_penalty`      | number                   |     | 0              | 반복 억제(같은 토큰/표현 반복을 줄임)                               |\n",
    "| `presence_penalty`       | number                   |     | 0              | 새로운 주제/토큰 등장 유도(이미 나온 토큰에 페널티)                       |\n",
    "| `logprobs`               | boolean | null           |     | false          | 토큰 로그확률 정보 포함 여부                                     |\n",
    "| `top_logprobs`           | integer | null           |     | -              | logprobs 사용 시, 각 토큰에 대해 상위 N개 후보 로그확률                |\n",
    "| `seed`                   | integer | null           |     | -              | 가능하면 결정적 결과를 유도(완전 보장 아님)                            |\n",
    "| `user`                   | string                   |     | -              | **Deprecated**(문서상 대체 필드 안내)                         |\n",
    "| `metadata`               | object                   |     | -              | 임의 메타데이터(추적/분류 목적)                                   |\n",
    "| `store`                  | boolean | null           |     | false          | 결과 저장 여부(저장된 completion은 삭제 API로 삭제 가능)              |\n",
    "| `audio`                  | object | null            |     | -              | 오디오 출력 설정(voice/format 등, 오디오 모달리티 사용 시)             |\n",
    "| `modalities`             | array                    |     | -              | 출력 모달리티 지정(예: `[\"text\"]`, `[\"audio\"]`)               |\n",
    "| `reasoning_effort`       | string                   |     | -              | reasoning 모델에서 추론 노력 수준 제어(모델 지원 시)                  |\n",
    "| `service_tier`           | string                   |     | -              | 처리 티어 지정(auto/default/priority/flex 등, 계정/모델 정책에 따름) |\n",
    "| `prediction`             | object                   |     | -              | Predicted Output 설정(미리 아는 출력이 많을 때 속도 최적화)           |\n",
    "| `prompt_cache_key`       | string                   |     | -              | 프롬프트 캐시 키(성능/비용 최적화 목적)                              |\n",
    "| `prompt_cache_retention` | string | number          |     | -              | 캐시 유지 정책(지원 범위는 문서/정책에 따름)                           |\n",
    "| `web_search_options`     | object                   |     | -              | 웹 검색 도구 사용 시 옵션(컨텍스트 크기/위치 근사 등, 지원 허용 시)            |\n",
    "| `verbosity`              | string                   |     | `medium`       | 출력 장황함 가이드(`low`/`medium`/`high`)                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPEN_AI_API_KEY')\n",
    "URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REST API 요청\n",
    "라이브러리 없이 직접 HTTP 통신을 통해 api를 호출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"당신은 친절한 AI 강사입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Chat Completions API가 뭐야? 2~3문장으로 답변해줘\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(URL, headers=headers, json=payload)\n",
    "pprint(response.json())\n",
    "print(response.json()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI SDK를 활용한 요청\n",
    "공식 라이브러리를 사용하여 생산성을 높이는 표준 방식이다.  \n",
    "`pip install openai` 를 통해 설치한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Openai SDK를 사용하면 어떤 점이 좋아?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompt 비교\n",
    "\n",
    "동일한 질문에 대해 AI의 페르소나에 따라 답변이 어떻게 달라지는지 확인해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"집에 있는데도 집에 가고 싶을 때 조언 해줘.\"\n",
    "\n",
    "personas = {\n",
    "    \"열정적인 셰프\": \"당신은 요리에 인생을 건 셰프입니다. 인생의 모든 이치를 요리 과정과 재료에 비유하여 설명하세요.\",\n",
    "    \"엄격한 헬스 트레이너\": \"당신은 매우 엄격한 운동 전문가입니다. 강한 어조로 자기관리를 강조하며 답변하세요.\",\n",
    "    \"지혜로운 판다\": \"당신은 대나무 숲에 사는 느긋하고 지혜로운 판다입니다. 느릿느릿하고 평화로운 말투로 조언을 건네세요.\"\n",
    "}\n",
    "\n",
    "for name, prompt in personas.items():\n",
    "    print(f\"--- [{name}] 버전 ---\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature 비교\n",
    "\n",
    "동일한 질문에 대해 temperature에 따라 답변이 어떻게 달라지는지 확인해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_topic = \"운동화 브랜드의 새로운 슬로건을 5개 제안해줘. 단, '속도'나 '승리' 같은 뻔한 단어는 제외하고 아주 기발하게 작성해줘.\"\n",
    "temperatures = [0.3, 0.8, 1.0, 1.3, 1.5, 1.6, 1.8]\n",
    "\n",
    "for t in temperatures:\n",
    "    print(f\"### 설정값 (Temperature): {t} ###\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": creative_topic}],\n",
    "        temperature=t,\n",
    "        max_completion_tokens=200, \n",
    "        timeout=15.0\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_topic = \"우리집 강아지의 별명을 3개 지어줘.\"\n",
    "temperatures = [0.3, 0.8, 1.0, 1.3, 1.5, 1.6, 1.8]\n",
    "\n",
    "for t in temperatures:\n",
    "    print(f\"### 설정값 (Temperature): {t} ###\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": creative_topic}],\n",
    "        temperature=t,\n",
    "        max_completion_tokens=200, \n",
    "        timeout=15.0\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `messages` 배열을 활용한 대화 맥락 유지 (Context Window)\n",
    "Chat Completions API는 상태를 저장하지 않는(Stateless) 방식이므로, 이전 대화 내역을 리스트에 계속 누적해서 보내야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_without_memory(user_input):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 3. 모델의 답변을 기록에 추가 (이것이 맥락 유지의 핵심)\n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# 실습 테스트\n",
    "print(\"Q1: 내 이름은 jun이야.\")\n",
    "print(f\"A1: {chat_without_memory('내 이름은 jun이야')}\\n\")\n",
    "\n",
    "print(\"Q2: 내 이름이 뭐라고?\")\n",
    "print(f\"A2: {chat_without_memory('내 이름이 뭐라고?')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 내역을 저장할 리스트 초기화\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"당신은 사용자의 이름을 기억하는 비서입니다.\"}\n",
    "]\n",
    "\n",
    "def chat_with_memory(user_input):\n",
    "    # 1. 사용자 질문을 기록에 추가\n",
    "    history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # 2. 전체 기록을 API에 전송\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=history\n",
    "    )\n",
    "    \n",
    "    # 3. 모델의 답변을 기록에 추가 (이것이 맥락 유지의 핵심)\n",
    "    answer = response.choices[0].message.content\n",
    "    history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# 실습 테스트\n",
    "print(\"Q1: 내 이름은 jun이야.\")\n",
    "print(f\"A1: {chat_with_memory('내 이름은 jun이야.')}\\n\")\n",
    "\n",
    "print(\"Q2: 내 이름이 뭐라고?\")\n",
    "print(f\"A2: {chat_with_memory('내 이름이 뭐라고?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Outputs (구조화된 출력)\n",
    "모델의 답변을 단순히 텍스트로 받는 것이 아니라, JSON 형태로 고정하여 받을 수 있다.  \n",
    "웹 서비스의 백엔드에서 데이터를 바로 처리해야 할 때 필수적인 기능이다.  \n",
    "여기서는 `JSON mode(json_object)`로 json format을 활용하지만,  \n",
    "이후에는 pydantic 라이브러리를 활용한 `JSON Scheme` 방식을 통해 명확한 json 응답 형식을 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint \n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"너는 요리사야. 답변은 반드시 JSON 형식으로 해줘.\"},\n",
    "        {\"role\": \"user\", \"content\": \"떡볶이 레시피 알려줘.\"}\n",
    "    ],\n",
    "    # JSON 모드 활성화\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "# 문자열로 온 답변을 직접 파싱해야 함\n",
    "res_json = json.loads(response.choices[0].message.content)\n",
    "print(res_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming (실시간 응답 처리)\n",
    "stream=True 설정을 통해 활성화한다.  \n",
    "서버는 SSE(Server-Sent Events) 프로토콜을 사용하여 응답을 끊지 않고 조각(Chunk) 단위로 지속적으로 전송한다.  \n",
    "응답 객체는 제너레이터 형식으로, for 루프를 사용해 활용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"양자 역학에 대해 초등학생도 이해할 수 있게 설명해줘.\"\n",
    "print(f\"질문: {prompt}\\n\")\n",
    "print(\"답변: \", end=\"\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    stream=True \n",
    ")\n",
    "\n",
    "full_response = \"\"\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True) # flush 옵션을 통해 출력 버퍼를 즉시 비워 스트리밍 답변이 지연 없이 실시간으로 표시되도록 한다.\n",
    "        full_response += content\n",
    "\n",
    "print(\"\\n\\n--- 스트리밍 종료 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비동기 요청\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "\n",
    "async_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "async def get_food_recommendation(city):\n",
    "    print(f\"[{city}] 맛집 검색 시작...\")\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"{city}에 가면 꼭 먹어야 할 음식 딱 한 가지만 추천해줘.\"}]\n",
    "    )\n",
    "    print(f\"[{city}] 검색 완료!\")\n",
    "    return f\"{city}: {response.choices[0].message.content}\"\n",
    "\n",
    "async def main():\n",
    "    cities = [\"서울\", \"파리\", \"뉴욕\", \"도쿄\", \"방콕\", \"로마\"]\n",
    "    tasks = [get_food_recommendation(c) for c in cities]\n",
    "    \n",
    "    # 여러 요청을 동시에(병렬로) 처리\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    print(\"\\n--- [여행자들을 위한 미식 가이드] ---\")\n",
    "    for r in results:\n",
    "        print(r)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logprobs - 확률 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "prompt = \"새로 오픈한 조용한 북카페 이름을 한글로 딱 하나만 추천해줘.\"\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    logprobs=True,\n",
    "    top_logprobs=3,\n",
    "    max_completion_tokens=50\n",
    ")\n",
    "\n",
    "content = response.choices[0].message.content\n",
    "logprobs_data = response.choices[0].logprobs.content\n",
    "\n",
    "print(f\"질문: {prompt}\")\n",
    "print(f\"답변: {content}\\n\")\n",
    "print(f\"{'Token':<15} | {'Probability':<12} | {'Top Alternatives'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for lp in logprobs_data:\n",
    "    prob = math.exp(lp.logprob) * 100\n",
    "    alternatives = [f\"{top.token}({math.exp(top.logprob)*100:.1f}%)\" for top in lp.top_logprobs]\n",
    "    print(f\"{lp.token:<15} | {prob:>10.2f}% | {', '.join(alternatives)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
